{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f0d388b14e0>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f0d3878dfd0>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f0d3878df60>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    " \n",
    "# Load pre-shuffled MNIST data into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],28, 28,1)\n",
    "X_test = X_test.reshape(X_test.shape[0],28, 28,1)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0d38089438>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0d38100240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline  \n",
    "plt.imshow(np.squeeze(X_train[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 26, 26, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenjennhaur/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/__main__.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
    "print(model.output_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenjennhaur/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/__main__.py:1: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "model.add(Convolution2D(32, 3, 3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 1472/60000 [..............................] - ETA: 6s - loss: 0.0159 - acc: 0.9932"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenjennhaur/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 6s - loss: 0.0166 - acc: 0.9946     \n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s - loss: 0.0167 - acc: 0.9949     \n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s - loss: 0.0161 - acc: 0.9947     \n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s - loss: 0.0148 - acc: 0.9953     \n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s - loss: 0.0153 - acc: 0.9948     \n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s - loss: 0.0152 - acc: 0.9951     \n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s - loss: 0.0146 - acc: 0.9953     \n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s - loss: 0.0133 - acc: 0.9957     \n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s - loss: 0.0140 - acc: 0.9955     \n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s - loss: 0.0143 - acc: 0.9953     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0cf4f70c18>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, \n",
    "          batch_size=32, nb_epoch=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.034837121467030968, 0.99209999999999998]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = np.loadtxt(\"pima-indians-diabetes.data\",delimiter=\",\")\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "768/768 [==============================] - 0s - loss: 1.5601 - acc: 0.6016     \n",
      "Epoch 2/150\n",
      "768/768 [==============================] - 0s - loss: 0.7222 - acc: 0.6419     \n",
      "Epoch 3/150\n",
      "768/768 [==============================] - 0s - loss: 0.6867 - acc: 0.6471     \n",
      "Epoch 4/150\n",
      "768/768 [==============================] - 0s - loss: 0.6754 - acc: 0.6497     \n",
      "Epoch 5/150\n",
      "768/768 [==============================] - 0s - loss: 0.6710 - acc: 0.6523     \n",
      "Epoch 6/150\n",
      "768/768 [==============================] - 0s - loss: 0.6673 - acc: 0.6536     \n",
      "Epoch 7/150\n",
      "768/768 [==============================] - 0s - loss: 0.6629 - acc: 0.6536     \n",
      "Epoch 8/150\n",
      "768/768 [==============================] - 0s - loss: 0.6601 - acc: 0.6536     \n",
      "Epoch 9/150\n",
      "768/768 [==============================] - 0s - loss: 0.6574 - acc: 0.6536     \n",
      "Epoch 10/150\n",
      "768/768 [==============================] - 0s - loss: 0.6553 - acc: 0.6549     \n",
      "Epoch 11/150\n",
      "768/768 [==============================] - 0s - loss: 0.6536 - acc: 0.6549     \n",
      "Epoch 12/150\n",
      "768/768 [==============================] - 0s - loss: 0.6521 - acc: 0.6563     \n",
      "Epoch 13/150\n",
      "768/768 [==============================] - 0s - loss: 0.6503 - acc: 0.6523     \n",
      "Epoch 14/150\n",
      "768/768 [==============================] - 0s - loss: 0.6486 - acc: 0.6536     \n",
      "Epoch 15/150\n",
      "768/768 [==============================] - 0s - loss: 0.6476 - acc: 0.6536     \n",
      "Epoch 16/150\n",
      "768/768 [==============================] - 0s - loss: 0.6474 - acc: 0.6536     \n",
      "Epoch 17/150\n",
      "768/768 [==============================] - 0s - loss: 0.6461 - acc: 0.6536     \n",
      "Epoch 18/150\n",
      "768/768 [==============================] - 0s - loss: 0.6459 - acc: 0.6563     \n",
      "Epoch 19/150\n",
      "768/768 [==============================] - 0s - loss: 0.6449 - acc: 0.6536     \n",
      "Epoch 20/150\n",
      "768/768 [==============================] - 0s - loss: 0.6445 - acc: 0.6549     \n",
      "Epoch 21/150\n",
      "768/768 [==============================] - 0s - loss: 0.6449 - acc: 0.6536     \n",
      "Epoch 22/150\n",
      "768/768 [==============================] - 0s - loss: 0.6442 - acc: 0.6615     \n",
      "Epoch 23/150\n",
      "768/768 [==============================] - 0s - loss: 0.6434 - acc: 0.6549     \n",
      "Epoch 24/150\n",
      "768/768 [==============================] - 0s - loss: 0.6424 - acc: 0.6536     \n",
      "Epoch 25/150\n",
      "768/768 [==============================] - 0s - loss: 0.6430 - acc: 0.6576     \n",
      "Epoch 26/150\n",
      "768/768 [==============================] - 0s - loss: 0.6424 - acc: 0.6563     \n",
      "Epoch 27/150\n",
      "768/768 [==============================] - 0s - loss: 0.6424 - acc: 0.6563     \n",
      "Epoch 28/150\n",
      "768/768 [==============================] - 0s - loss: 0.6421 - acc: 0.6576     \n",
      "Epoch 29/150\n",
      "768/768 [==============================] - 0s - loss: 0.6418 - acc: 0.6602     \n",
      "Epoch 30/150\n",
      "768/768 [==============================] - 0s - loss: 0.6418 - acc: 0.6549     \n",
      "Epoch 31/150\n",
      "768/768 [==============================] - 0s - loss: 0.6410 - acc: 0.6563     \n",
      "Epoch 32/150\n",
      "768/768 [==============================] - 0s - loss: 0.6413 - acc: 0.6549     \n",
      "Epoch 33/150\n",
      "768/768 [==============================] - 0s - loss: 0.6416 - acc: 0.6576     \n",
      "Epoch 34/150\n",
      "768/768 [==============================] - 0s - loss: 0.6410 - acc: 0.6563     \n",
      "Epoch 35/150\n",
      "768/768 [==============================] - 0s - loss: 0.6411 - acc: 0.6628     \n",
      "Epoch 36/150\n",
      "768/768 [==============================] - 0s - loss: 0.6407 - acc: 0.6563     \n",
      "Epoch 37/150\n",
      "768/768 [==============================] - 0s - loss: 0.6403 - acc: 0.6589     \n",
      "Epoch 38/150\n",
      "768/768 [==============================] - 0s - loss: 0.6404 - acc: 0.6589     \n",
      "Epoch 39/150\n",
      "768/768 [==============================] - 0s - loss: 0.6406 - acc: 0.6563     \n",
      "Epoch 40/150\n",
      "768/768 [==============================] - 0s - loss: 0.6397 - acc: 0.6576     \n",
      "Epoch 41/150\n",
      "768/768 [==============================] - 0s - loss: 0.6418 - acc: 0.6549     \n",
      "Epoch 42/150\n",
      "768/768 [==============================] - 0s - loss: 0.6409 - acc: 0.6549     \n",
      "Epoch 43/150\n",
      "768/768 [==============================] - 0s - loss: 0.6407 - acc: 0.6615     \n",
      "Epoch 44/150\n",
      "768/768 [==============================] - 0s - loss: 0.6407 - acc: 0.6589     \n",
      "Epoch 45/150\n",
      "768/768 [==============================] - 0s - loss: 0.6407 - acc: 0.6549     \n",
      "Epoch 46/150\n",
      "768/768 [==============================] - 0s - loss: 0.6398 - acc: 0.6589     \n",
      "Epoch 47/150\n",
      "768/768 [==============================] - 0s - loss: 0.6400 - acc: 0.6589     \n",
      "Epoch 48/150\n",
      "768/768 [==============================] - 0s - loss: 0.6398 - acc: 0.6615     \n",
      "Epoch 49/150\n",
      "768/768 [==============================] - 0s - loss: 0.6403 - acc: 0.6563     \n",
      "Epoch 50/150\n",
      "768/768 [==============================] - 0s - loss: 0.6394 - acc: 0.6602     \n",
      "Epoch 51/150\n",
      "768/768 [==============================] - 0s - loss: 0.6410 - acc: 0.6536     \n",
      "Epoch 52/150\n",
      "768/768 [==============================] - 0s - loss: 0.6404 - acc: 0.6602     \n",
      "Epoch 53/150\n",
      "768/768 [==============================] - 0s - loss: 0.6403 - acc: 0.6602     \n",
      "Epoch 54/150\n",
      "768/768 [==============================] - 0s - loss: 0.6400 - acc: 0.6563     \n",
      "Epoch 55/150\n",
      "768/768 [==============================] - 0s - loss: 0.6403 - acc: 0.6602     \n",
      "Epoch 56/150\n",
      "768/768 [==============================] - 0s - loss: 0.6401 - acc: 0.6589     \n",
      "Epoch 57/150\n",
      "768/768 [==============================] - 0s - loss: 0.6397 - acc: 0.6602     \n",
      "Epoch 58/150\n",
      "768/768 [==============================] - 0s - loss: 0.6390 - acc: 0.6576     \n",
      "Epoch 59/150\n",
      "768/768 [==============================] - 0s - loss: 0.6392 - acc: 0.6576     \n",
      "Epoch 60/150\n",
      "768/768 [==============================] - 0s - loss: 0.6399 - acc: 0.6589     \n",
      "Epoch 61/150\n",
      "768/768 [==============================] - 0s - loss: 0.6402 - acc: 0.6576     \n",
      "Epoch 62/150\n",
      "768/768 [==============================] - 0s - loss: 0.6402 - acc: 0.6576     \n",
      "Epoch 63/150\n",
      "768/768 [==============================] - 0s - loss: 0.6394 - acc: 0.6589     \n",
      "Epoch 64/150\n",
      "768/768 [==============================] - 0s - loss: 0.6396 - acc: 0.6576     \n",
      "Epoch 65/150\n",
      "768/768 [==============================] - 0s - loss: 0.6387 - acc: 0.6576     \n",
      "Epoch 66/150\n",
      "768/768 [==============================] - 0s - loss: 0.6388 - acc: 0.6602     \n",
      "Epoch 67/150\n",
      "768/768 [==============================] - 0s - loss: 0.6389 - acc: 0.6602     \n",
      "Epoch 68/150\n",
      "768/768 [==============================] - 0s - loss: 0.6393 - acc: 0.6602     \n",
      "Epoch 69/150\n",
      "768/768 [==============================] - 0s - loss: 0.6399 - acc: 0.6615     \n",
      "Epoch 70/150\n",
      "768/768 [==============================] - 0s - loss: 0.6388 - acc: 0.6576     \n",
      "Epoch 71/150\n",
      "768/768 [==============================] - 0s - loss: 0.6391 - acc: 0.6602     \n",
      "Epoch 72/150\n",
      "768/768 [==============================] - 0s - loss: 0.6390 - acc: 0.6602     \n",
      "Epoch 73/150\n",
      "768/768 [==============================] - 0s - loss: 0.6392 - acc: 0.6589     \n",
      "Epoch 74/150\n",
      "768/768 [==============================] - 0s - loss: 0.6388 - acc: 0.6602     \n",
      "Epoch 75/150\n",
      "768/768 [==============================] - 0s - loss: 0.6394 - acc: 0.6589     \n",
      "Epoch 76/150\n",
      "768/768 [==============================] - 0s - loss: 0.6380 - acc: 0.6602     \n",
      "Epoch 77/150\n",
      "768/768 [==============================] - 0s - loss: 0.6395 - acc: 0.6602     \n",
      "Epoch 78/150\n",
      "768/768 [==============================] - 0s - loss: 0.6382 - acc: 0.6615     \n",
      "Epoch 79/150\n",
      "768/768 [==============================] - 0s - loss: 0.6378 - acc: 0.6602     \n",
      "Epoch 80/150\n",
      "768/768 [==============================] - 0s - loss: 0.6384 - acc: 0.6589     \n",
      "Epoch 81/150\n",
      "768/768 [==============================] - 0s - loss: 0.6396 - acc: 0.6576     \n",
      "Epoch 82/150\n",
      "768/768 [==============================] - 0s - loss: 0.6385 - acc: 0.6602     \n",
      "Epoch 83/150\n",
      "768/768 [==============================] - 0s - loss: 0.6384 - acc: 0.6589     \n",
      "Epoch 84/150\n",
      "768/768 [==============================] - 0s - loss: 0.6383 - acc: 0.6602     \n",
      "Epoch 85/150\n",
      "768/768 [==============================] - 0s - loss: 0.6393 - acc: 0.6589     \n",
      "Epoch 86/150\n",
      "768/768 [==============================] - 0s - loss: 0.6377 - acc: 0.6615     \n",
      "Epoch 87/150\n",
      "768/768 [==============================] - 0s - loss: 0.6393 - acc: 0.6589     \n",
      "Epoch 88/150\n",
      "768/768 [==============================] - 0s - loss: 0.6381 - acc: 0.6628     \n",
      "Epoch 89/150\n",
      "768/768 [==============================] - 0s - loss: 0.6384 - acc: 0.6602     \n",
      "Epoch 90/150\n",
      "768/768 [==============================] - 0s - loss: 0.6381 - acc: 0.6602     \n",
      "Epoch 91/150\n",
      "768/768 [==============================] - 0s - loss: 0.6384 - acc: 0.6615     \n",
      "Epoch 92/150\n",
      "768/768 [==============================] - 0s - loss: 0.6379 - acc: 0.6589     \n",
      "Epoch 93/150\n",
      "768/768 [==============================] - 0s - loss: 0.6385 - acc: 0.6589     \n",
      "Epoch 94/150\n",
      "768/768 [==============================] - 0s - loss: 0.6403 - acc: 0.6576     \n",
      "Epoch 95/150\n",
      "768/768 [==============================] - 0s - loss: 0.6390 - acc: 0.6589     \n",
      "Epoch 96/150\n",
      "768/768 [==============================] - 0s - loss: 0.6379 - acc: 0.6602     \n",
      "Epoch 97/150\n",
      "768/768 [==============================] - 0s - loss: 0.6392 - acc: 0.6576     \n",
      "Epoch 98/150\n",
      "768/768 [==============================] - 0s - loss: 0.6384 - acc: 0.6589     \n",
      "Epoch 99/150\n",
      "768/768 [==============================] - 0s - loss: 0.6370 - acc: 0.6641     \n",
      "Epoch 100/150\n",
      "768/768 [==============================] - 0s - loss: 0.6417 - acc: 0.6589     \n",
      "Epoch 101/150\n",
      "768/768 [==============================] - 0s - loss: 0.6408 - acc: 0.6615     \n",
      "Epoch 102/150\n",
      "768/768 [==============================] - 0s - loss: 0.6318 - acc: 0.6654     \n",
      "Epoch 103/150\n",
      "768/768 [==============================] - 0s - loss: 0.6305 - acc: 0.6693     \n",
      "Epoch 104/150\n",
      "768/768 [==============================] - 0s - loss: 0.6294 - acc: 0.6693     \n",
      "Epoch 105/150\n",
      "768/768 [==============================] - 0s - loss: 0.6381 - acc: 0.6589     \n",
      "Epoch 106/150\n",
      "768/768 [==============================] - 0s - loss: 0.6299 - acc: 0.6719     \n",
      "Epoch 107/150\n",
      "768/768 [==============================] - 0s - loss: 0.6323 - acc: 0.6654     \n",
      "Epoch 108/150\n",
      "768/768 [==============================] - 0s - loss: 0.6272 - acc: 0.6732     \n",
      "Epoch 109/150\n",
      "768/768 [==============================] - 0s - loss: 0.6259 - acc: 0.6732     \n",
      "Epoch 110/150\n",
      "768/768 [==============================] - 0s - loss: 0.6254 - acc: 0.6719     \n",
      "Epoch 111/150\n",
      "768/768 [==============================] - 0s - loss: 0.6226 - acc: 0.6771     \n",
      "Epoch 112/150\n",
      "768/768 [==============================] - 0s - loss: 0.6227 - acc: 0.6745     \n",
      "Epoch 113/150\n",
      "768/768 [==============================] - 0s - loss: 0.6290 - acc: 0.6693     \n",
      "Epoch 114/150\n",
      "768/768 [==============================] - 0s - loss: 0.6244 - acc: 0.6680     \n",
      "Epoch 115/150\n",
      "768/768 [==============================] - 0s - loss: 0.6208 - acc: 0.6849     \n",
      "Epoch 116/150\n",
      "768/768 [==============================] - 0s - loss: 0.6248 - acc: 0.6732     \n",
      "Epoch 117/150\n",
      "768/768 [==============================] - 0s - loss: 0.6241 - acc: 0.6719     \n",
      "Epoch 118/150\n",
      "768/768 [==============================] - 0s - loss: 0.6248 - acc: 0.6758     \n",
      "Epoch 119/150\n",
      "768/768 [==============================] - 0s - loss: 0.6226 - acc: 0.6849     \n",
      "Epoch 120/150\n",
      "768/768 [==============================] - 0s - loss: 0.6198 - acc: 0.6823     \n",
      "Epoch 121/150\n",
      "768/768 [==============================] - 0s - loss: 0.6199 - acc: 0.6810     \n",
      "Epoch 122/150\n",
      "768/768 [==============================] - 0s - loss: 0.6258 - acc: 0.6719     \n",
      "Epoch 123/150\n",
      "768/768 [==============================] - 0s - loss: 0.6171 - acc: 0.6810     \n",
      "Epoch 124/150\n",
      "768/768 [==============================] - 0s - loss: 0.6167 - acc: 0.6784     \n",
      "Epoch 125/150\n",
      "768/768 [==============================] - 0s - loss: 0.6126 - acc: 0.6888     \n",
      "Epoch 126/150\n",
      "768/768 [==============================] - 0s - loss: 0.6137 - acc: 0.6927     \n",
      "Epoch 127/150\n",
      "768/768 [==============================] - 0s - loss: 0.6154 - acc: 0.6810     \n",
      "Epoch 128/150\n",
      "768/768 [==============================] - 0s - loss: 0.6170 - acc: 0.6862     \n",
      "Epoch 129/150\n",
      "768/768 [==============================] - 0s - loss: 0.6131 - acc: 0.6836     \n",
      "Epoch 130/150\n",
      "768/768 [==============================] - 0s - loss: 0.6024 - acc: 0.7018     \n",
      "Epoch 131/150\n",
      "768/768 [==============================] - 0s - loss: 0.6089 - acc: 0.6862     \n",
      "Epoch 132/150\n",
      "768/768 [==============================] - 0s - loss: 0.6022 - acc: 0.7005     \n",
      "Epoch 133/150\n",
      "768/768 [==============================] - 0s - loss: 0.6003 - acc: 0.7070     \n",
      "Epoch 134/150\n",
      "768/768 [==============================] - 0s - loss: 0.5957 - acc: 0.7005     \n",
      "Epoch 135/150\n",
      "768/768 [==============================] - 0s - loss: 0.5903 - acc: 0.7096     \n",
      "Epoch 136/150\n",
      "768/768 [==============================] - 0s - loss: 0.5883 - acc: 0.7240     \n",
      "Epoch 137/150\n",
      "768/768 [==============================] - 0s - loss: 0.5868 - acc: 0.7031     \n",
      "Epoch 138/150\n",
      "768/768 [==============================] - 0s - loss: 0.5828 - acc: 0.7070     \n",
      "Epoch 139/150\n",
      "768/768 [==============================] - 0s - loss: 0.5808 - acc: 0.7109     \n",
      "Epoch 140/150\n",
      "768/768 [==============================] - 0s - loss: 0.5763 - acc: 0.7292     \n",
      "Epoch 141/150\n",
      "768/768 [==============================] - 0s - loss: 0.5761 - acc: 0.7214     \n",
      "Epoch 142/150\n",
      "768/768 [==============================] - 0s - loss: 0.5734 - acc: 0.7240     \n",
      "Epoch 143/150\n",
      "768/768 [==============================] - 0s - loss: 0.5690 - acc: 0.7266     \n",
      "Epoch 144/150\n",
      "768/768 [==============================] - 0s - loss: 0.5717 - acc: 0.7305     \n",
      "Epoch 145/150\n",
      "768/768 [==============================] - 0s - loss: 0.5738 - acc: 0.7109     \n",
      "Epoch 146/150\n",
      "768/768 [==============================] - 0s - loss: 0.5705 - acc: 0.7201     \n",
      "Epoch 147/150\n",
      "768/768 [==============================] - 0s - loss: 0.5648 - acc: 0.7318     \n",
      "Epoch 148/150\n",
      "768/768 [==============================] - 0s - loss: 0.5644 - acc: 0.7266     \n",
      "Epoch 149/150\n",
      "768/768 [==============================] - 0s - loss: 0.5650 - acc: 0.7201     \n",
      "Epoch 150/150\n",
      "768/768 [==============================] - 0s - loss: 0.5599 - acc: 0.7331     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0d5c05a9e8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X, Y, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/768 [>.............................] - ETA: 0s\n",
      "acc: 72.53%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12507199801395106262\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 163119104\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 11541265819684223858\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
